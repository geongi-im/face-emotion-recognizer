import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from dotenv import load_dotenv
from logger_config import get_logger

load_dotenv()

# ============ ì‹œìŠ¤í…œ ì„¤ì • (ì½”ë“œ ìƒìˆ˜) ============
# ì´ ê°’ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ê³ ì •ë˜ë©°, í•„ìš”ì‹œ ì½”ë“œì—ì„œ ì§ì ‘ ìˆ˜ì •
USE_GPU = True  # GPU ì‚¬ìš© ì—¬ë¶€ (Falseë¡œ ë³€ê²½í•˜ë©´ CPUë§Œ ì‚¬ìš©)
ENABLE_VISUALIZATION = True  # ì‹œê°í™” í™œì„±í™” (CLI í™˜ê²½ì—ì„œëŠ” Falseë¡œ ë³€ê²½)
BACKBONE = 'MobileNetV3Large'  # ë°±ë³¸ ëª¨ë¸
SEED = 42  # ëœë¤ ì‹œë“œ

# í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ê²€ì¦
REQUIRED_ENV_VARS = [
    # ë°ì´í„°ì…‹
    'CLASS_NAMES',
    'DATA_DIR_TRAIN',
    'DATA_DIR_VAL',
    # ì´ë¯¸ì§€ ë° ë°°ì¹˜
    'IMG_SIZE',
    'BATCH_SIZE',
    'VAL_STEPS',
    # í—¤ë“œ í•™ìŠµ
    'STEPS_PER_EPOCH_HEAD',
    'EPOCHS_HEAD',
    'LEARNING_RATE_HEAD',
    # íŒŒì¸íŠœë‹
    'STEPS_PER_EPOCH_FINETUNE',
    'EPOCHS_FINETUNE',
    'LEARNING_RATE_FINETUNE',
    # ì •ê·œí™”
    'DROPOUT_RATE',
    'WEIGHT_DECAY',
    # ì½œë°±
    'EARLY_STOPPING_PATIENCE',
    'REDUCE_LR_PATIENCE',
    'REDUCE_LR_FACTOR',
    # ê³ ê¸‰ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    'FREEZE_BN_MODE',
    'WARMUP_RATIO',
    'AUTO_STEPS',
    'DISABLE_MIX_IN_LAST_STAGE',
    'EMA_DECAY',
    # ë°ì´í„° ì¦ê°• (MixUp/CutMix)
    'MIXUP_ALPHA',
    'CUTMIX_ALPHA',
    'P_MIXUP',
    'P_CUTMIX',
]

missing_vars = []
for var in REQUIRED_ENV_VARS:
    if os.getenv(var) is None:
        missing_vars.append(var)

if missing_vars:
    print("âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:")
    for var in missing_vars:
        print(f"   - {var}")
    print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
    print("   1. .env.exampleì„ ë³µì‚¬í•˜ì—¬ .env íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”")
    print("   2. cp .env.example .env (Linux/Mac) ë˜ëŠ” copy .env.example .env (Windows)")
    print("   3. .env íŒŒì¼ì—ì„œ í•„ìš”í•œ ê°’ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”")
    exit(1)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
AUTO_STEPS = os.getenv('AUTO_STEPS') == 'true'   # ë°ì´í„°ì…‹ í¬ê¸°ë¡œ ìë™ ìŠ¤í… ê³„ì‚°
FREEZE_BN_MODE = os.getenv('FREEZE_BN_MODE')  # BN ë™ê²° ì „ëµ: all|s3|adaptive|none
WEIGHT_DECAY = float(os.getenv('WEIGHT_DECAY'))  # L2 ì •ê·œí™” ê°•ë„
WARMUP_RATIO = float(os.getenv('WARMUP_RATIO'))    # Warmup ë¹„ìœ¨ (0~1)

# í´ë˜ìŠ¤ëª… íŒŒì‹± (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
CLASS_NAMES_RAW = os.getenv('CLASS_NAMES')
CLASS_NAMES = [name.strip() for name in CLASS_NAMES_RAW.split(',') if name.strip()]

# í´ë˜ìŠ¤ëª… ê²€ì¦ (logger ìƒì„± ì´ì „ì— ì¡°ê¸° ì‹¤íŒ¨)
NUM_CLASSES = len(CLASS_NAMES)
if NUM_CLASSES < 1:
    print("âŒ í´ë˜ìŠ¤ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ .env íŒŒì¼ì—ì„œ CLASS_NAMESë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    print("   ì˜ˆì‹œ: CLASS_NAMES=happy,sad,angry")
    exit(1)

# í´ë˜ìŠ¤ëª…ì— ë¹ˆ ë¬¸ìì—´ ê²€ì¦ (ì¤‘ë³µ ê²€ì¦ì€ ì´ë¯¸ ìœ„ì—ì„œ ìˆ˜í–‰ë¨)
if not all(CLASS_NAMES):
    print("âŒ ë¹ˆ í´ë˜ìŠ¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ .env íŒŒì¼ì˜ CLASS_NAMESì—ì„œ ë¹ˆ ê°’ì„ ì œê±°í•˜ì„¸ìš”.")
    exit(1)

if len(CLASS_NAMES) != len(set(CLASS_NAMES)):
    print("âŒ ì¤‘ë³µëœ í´ë˜ìŠ¤ëª…ì´ ìˆìŠµë‹ˆë‹¤.")
    print(f"   ì„¤ì •ëœ í´ë˜ìŠ¤: {CLASS_NAMES}")
    print("ğŸ’¡ .env íŒŒì¼ì˜ CLASS_NAMESì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ì„¸ìš”.")
    exit(1)

# ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
DATA_DIR_TRAIN = os.getenv('DATA_DIR_TRAIN')
DATA_DIR_VAL = os.getenv('DATA_DIR_VAL')

IMG_SIZE = int(os.getenv('IMG_SIZE'))
BATCH_SIZE = int(os.getenv('BATCH_SIZE'))
VAL_STEPS = int(os.getenv('VAL_STEPS'))

STEPS_PER_EPOCH_HEAD = int(os.getenv('STEPS_PER_EPOCH_HEAD'))
EPOCHS_HEAD = int(os.getenv('EPOCHS_HEAD'))
LEARNING_RATE_HEAD = float(os.getenv('LEARNING_RATE_HEAD'))

STEPS_PER_EPOCH_FINETUNE = int(os.getenv('STEPS_PER_EPOCH_FINETUNE'))
EPOCHS_FINETUNE = int(os.getenv('EPOCHS_FINETUNE'))
LEARNING_RATE_FINETUNE = float(os.getenv('LEARNING_RATE_FINETUNE'))

DROPOUT_RATE = float(os.getenv('DROPOUT_RATE'))
EARLY_STOPPING_PATIENCE = int(os.getenv('EARLY_STOPPING_PATIENCE'))
REDUCE_LR_PATIENCE = int(os.getenv('REDUCE_LR_PATIENCE'))
REDUCE_LR_FACTOR = float(os.getenv('REDUCE_LR_FACTOR'))
DISABLE_MIX_IN_LAST_STAGE = os.getenv('DISABLE_MIX_IN_LAST_STAGE') == 'true'

# MixUp/CutMix í•˜ì´í¼íŒŒë¼ë¯¸í„°
MIXUP_ALPHA = float(os.getenv('MIXUP_ALPHA'))
CUTMIX_ALPHA = float(os.getenv('CUTMIX_ALPHA'))
P_MIXUP = float(os.getenv('P_MIXUP'))
P_CUTMIX = float(os.getenv('P_CUTMIX'))

# Label smoothing: MixUp/CutMixì™€ ì¤‘ë³µ ê·œì œ ë°©ì§€ ìœ„í•´ ê¸°ë³¸ê°’ 0.0ë¡œ ì¡°ì •
# í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥
LABEL_SMOOTHING = os.getenv('LABEL_SMOOTHING')
if LABEL_SMOOTHING is None:
    default_smoothing = 0.0 if (P_MIXUP > 0 or P_CUTMIX > 0) else 0.05
    LABEL_SMOOTHING = default_smoothing
else:
    LABEL_SMOOTHING = float(LABEL_SMOOTHING)
logger = get_logger(__name__)
logger.info(f"ğŸ”§ Loss label smoothing: {LABEL_SMOOTHING}")

# ê³ ì • ê²½ë¡œ ì„¤ì •
OUT_DIR = "models"

# ì‹œê°í™” í™œì„±í™” ì‹œì—ë§Œ matplotlib import
if ENABLE_VISUALIZATION:
    import matplotlib
    matplotlib.use('TkAgg')  # GUI ë°±ì—”ë“œ ì„¤ì • (í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    
    # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (Noto Sans Korean ì‚¬ìš©)
    font_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'fonts')
    noto_font_path = os.path.join(font_path, 'NotoSansKR-Regular.ttf')
    
    if os.path.exists(noto_font_path):
        try:
            fm.fontManager.addfont(noto_font_path)
            font_name = fm.FontProperties(fname=noto_font_path).get_name()
            plt.rcParams['font.family'] = [font_name]
            print(f"âœ… í•œê¸€ í°íŠ¸ ë¡œë“œ ì„±ê³µ: {font_name}")
        except Exception as e:
            print(f"âš ï¸  Noto Sans Korean í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            plt.rcParams['font.family'] = ['DejaVu Sans']
    else:
        print("âš ï¸  Noto Sans Korean í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ fonts/NotoSansKR-Regular.ttf íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        print("   ë‹¤ìš´ë¡œë“œ: https://fonts.google.com/noto/specimen/Noto+Sans+KR")
        plt.rcParams['font.family'] = ['DejaVu Sans']
    
    plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.ion()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ í™œì„±í™”

# ë¡œê¹… ì„¤ì •
def get_backbone_and_preprocess(img_size: int):
    """
    ë°±ë³¸ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ í•¨ìˆ˜ ë°˜í™˜ (MobileNetV3Large ê³ ì •)

    Args:
        img_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°

    Returns:
        (model_fn, preprocess_fn): ëª¨ë¸ ìƒì„± í•¨ìˆ˜ì™€ ì „ì²˜ë¦¬ í•¨ìˆ˜
    """
    input_shape = (img_size, img_size, 3)

    model_fn = lambda: tf.keras.applications.MobileNetV3Large(
        include_top=False,
        weights='imagenet',
        input_shape=input_shape
    )
    preprocess_fn = tf.keras.applications.mobilenet_v3.preprocess_input

    return model_fn, preprocess_fn

# ì‹œê°í™” ìƒíƒœ ë¡œê¹…
if ENABLE_VISUALIZATION:
    logger.info("âœ… ì‹¤ì‹œê°„ ì‹œê°í™” ê¸°ëŠ¥ í™œì„±í™”")
else:
    logger.info("âŒ ì‹œê°í™” ê¸°ëŠ¥ ë¹„í™œì„±í™” (CLI í™˜ê²½ ë˜ëŠ” ì„¤ì •ì— ì˜í•´)")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ë¡œê¹…
logger.info("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •:")
logger.info(f"   - GPU ì‚¬ìš©: {'âœ… í™œì„±í™”' if USE_GPU else 'âŒ ë¹„í™œì„±í™”'}")
logger.info(f"   - ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}x{IMG_SIZE}")
logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
logger.info(f"   - ê²€ì¦ ìŠ¤í…: {VAL_STEPS}")
logger.info(f"   - ë°±ë³¸: {BACKBONE}")
logger.info(f"   - í—¤ë“œ í•™ìŠµ ì—í¬í¬: {EPOCHS_HEAD}")
logger.info(f"   - í—¤ë“œ ìŠ¤í…/ì—í¬í¬: {STEPS_PER_EPOCH_HEAD}")
logger.info(f"   - í—¤ë“œ í•™ìŠµë¥ : {LEARNING_RATE_HEAD}")
logger.info(f"   - íŒŒì¸íŠœë‹ ì—í¬í¬: {EPOCHS_FINETUNE}")
logger.info(f"   - íŒŒì¸íŠœë‹ ìŠ¤í…/ì—í¬í¬: {STEPS_PER_EPOCH_FINETUNE}")
logger.info(f"   - íŒŒì¸íŠœë‹ í•™ìŠµë¥ : {LEARNING_RATE_FINETUNE}")
logger.info(f"   - ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨: {DROPOUT_RATE}")
logger.info(f"   - MixUp: p={P_MIXUP}, alpha={MIXUP_ALPHA}")
logger.info(f"   - CutMix: p={P_CUTMIX}, alpha={CUTMIX_ALPHA}")
logger.info("ğŸ”„ ì½œë°± ì„¤ì •:")
logger.info(f"   - Early Stopping patience: {EARLY_STOPPING_PATIENCE}")
logger.info(f"   - í•™ìŠµë¥  ê°ì†Œ patience: {REDUCE_LR_PATIENCE}")
logger.info(f"   - í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨: {REDUCE_LR_FACTOR}")

class SimpleRealTimeCallback(tf.keras.callbacks.Callback):
    """ê°„ë‹¨í•œ ì‹¤ì‹œê°„ ì‹œê°í™” ì½œë°± (ì¡°ê±´ë¶€ ì‹¤í–‰)"""
    def __init__(self, stage_name="í•™ìŠµ"):
        super().__init__()
        self.stage_name = stage_name
        self.losses = []
        self.val_losses = []
        self.accuracies = []
        self.val_accuracies = []
        self.enabled = ENABLE_VISUALIZATION
        self.fig = None
        
        if self.enabled:
            # ê·¸ë˜í”„ ì°½ ì´ˆê¸°í™”
            self.fig = plt.figure(figsize=(12, 5))
            self.fig.canvas.manager.set_window_title(f'í›ˆë ¨ ì§„í–‰ ìƒí™© - {stage_name}')
            logger.info(f"ğŸ¯ {stage_name} í…ìŠ¤íŠ¸ + ê·¸ë˜í”„ ì‹œê°í™” ì¤€ë¹„ ì™„ë£Œ")
        else:
            logger.info(f"ğŸ“Š {stage_name} í…ìŠ¤íŠ¸ ì§„í–‰ ìƒí™©ë§Œ í‘œì‹œ")
        
    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            return
            
        # ë°ì´í„° ìˆ˜ì§‘ (ì‹œê°í™” ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ í•­ìƒ ìˆ˜ì§‘)
        self.losses.append(logs.get('loss', 0))
        self.val_losses.append(logs.get('val_loss', 0))
        # ì •í™•ë„ ìˆ˜ì§‘ (í•­ìƒ ìˆ˜ì§‘)
        self.accuracies.append(logs.get('accuracy', 0))
        self.val_accuracies.append(logs.get('val_accuracy', 0))
        
        # í…ìŠ¤íŠ¸ ì§„í–‰ ìƒí™©ì€ í•­ìƒ í‘œì‹œ (ë§¤ ì—í¬í¬)
        self.print_progress(epoch + 1, logs)
        
        # ì‹œê°í™”ê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ê·¸ë˜í”„ í‘œì‹œ (ë§¤ ì—í¬í¬)
        if self.enabled:
            self.plot_progress(epoch + 1)
    
    def plot_progress(self, current_epoch):
        """ì§„í–‰ ìƒí™© ê·¸ë˜í”„ í‘œì‹œ"""
        if not self.enabled or self.fig is None:
            return
            
        # ë””ë²„ê¹…: ë°ì´í„° í™•ì¸
        logger.info(f"ğŸ” [{self.stage_name}] ì—í¬í¬ {current_epoch} ê·¸ë˜í”„ ë°ì´í„°:")
        logger.info(f"   - ì†ì‹¤ ë°ì´í„°: {len(self.losses)}ê°œ {self.losses[-3:] if len(self.losses) >= 3 else self.losses}")
        logger.info(f"   - ê²€ì¦ ì†ì‹¤: {len(self.val_losses)}ê°œ {self.val_losses[-3:] if len(self.val_losses) >= 3 else self.val_losses}")
        logger.info(f"   - ì •í™•ë„: {len(self.accuracies)}ê°œ {self.accuracies[-3:] if len(self.accuracies) >= 3 else self.accuracies}")
        logger.info(f"   - ê²€ì¦ ì •í™•ë„: {len(self.val_accuracies)}ê°œ {self.val_accuracies[-3:] if len(self.val_accuracies) >= 3 else self.val_accuracies}")
            
        # ê¸°ì¡´ ê·¸ë˜í”„ ì§€ìš°ê¸°
        self.fig.clear()
        epochs = range(1, current_epoch + 1)
        
        # Loss ì„œë¸Œí”Œë¡¯
        ax1 = self.fig.add_subplot(1, 2, 1)
        
        # í›ˆë ¨ ì†ì‹¤ - íŒŒë€ìƒ‰ìœ¼ë¡œ ë¨¼ì € ê·¸ë¦¬ê¸°
        if self.losses:
            ax1.plot(epochs, self.losses, 'b-', label='í›ˆë ¨ ì†ì‹¤', linewidth=3, marker='o', markersize=4)
            logger.info(f"âœ… í›ˆë ¨ ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ì™„ë£Œ: ê°’ ë²”ìœ„ {min(self.losses):.4f}~{max(self.losses):.4f}")
        
        # ê²€ì¦ ì†ì‹¤ - ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ë‚˜ì¤‘ì— ê·¸ë¦¬ê¸°
        if self.val_losses:
            ax1.plot(epochs, self.val_losses, 'r-', label='ê²€ì¦ ì†ì‹¤', linewidth=3, marker='s', markersize=4)
            logger.info(f"âœ… ê²€ì¦ ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ì™„ë£Œ: ê°’ ë²”ìœ„ {min(self.val_losses):.4f}~{max(self.val_losses):.4f}")
        
        ax1.set_title(f'{self.stage_name} - ì†ì‹¤', fontsize=12, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ì†ì‹¤ ì¶• ë²”ìœ„ ìë™ ì¡°ì •
        if self.losses or self.val_losses:
            all_losses = []
            if self.losses:
                all_losses.extend(self.losses)
            if self.val_losses:
                all_losses.extend(self.val_losses)
            
            if all_losses:
                min_loss = min(all_losses)
                max_loss = max(all_losses)
                margin = (max_loss - min_loss) * 0.1 if max_loss > min_loss else 0.1
                ax1.set_ylim([max(0, min_loss - margin), max_loss + margin])
        
        # Accuracy ì„œë¸Œí”Œë¡¯
        ax2 = self.fig.add_subplot(1, 2, 2)
        
        # í›ˆë ¨ ì •í™•ë„ - íŒŒë€ìƒ‰ìœ¼ë¡œ ë¨¼ì € ê·¸ë¦¬ê¸°
        if self.accuracies:
            ax2.plot(epochs, self.accuracies, 'b-', label='í›ˆë ¨ ì •í™•ë„', linewidth=3, marker='o', markersize=4)
            logger.info(f"âœ… í›ˆë ¨ ì •í™•ë„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ì™„ë£Œ: ê°’ ë²”ìœ„ {min(self.accuracies):.4f}~{max(self.accuracies):.4f}")
        
        # ê²€ì¦ ì •í™•ë„ - ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ë‚˜ì¤‘ì— ê·¸ë¦¬ê¸°
        if self.val_accuracies:
            ax2.plot(epochs, self.val_accuracies, 'r-', label='ê²€ì¦ ì •í™•ë„', linewidth=3, marker='s', markersize=4)
            logger.info(f"âœ… ê²€ì¦ ì •í™•ë„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ì™„ë£Œ: ê°’ ë²”ìœ„ {min(self.val_accuracies):.4f}~{max(self.val_accuracies):.4f}")
        
        ax2.set_title(f'{self.stage_name} - ì •í™•ë„', fontsize=12, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 1])
        
        self.fig.tight_layout()
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()
        plt.pause(0.1)  # ì—…ë°ì´íŠ¸ ê°„ê²© ëŠ˜ë¦¼
    
    def print_progress(self, current_epoch, logs):
        """CLI í™˜ê²½ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì§„í–‰ ìƒí™© í‘œì‹œ"""
        loss = logs.get('loss', 0)
        val_loss = logs.get('val_loss', 0)
        
        progress_msg = f"ğŸ“ˆ [{self.stage_name}] ì—í¬í¬ {current_epoch} | ì†ì‹¤: {loss:.4f}"
        if val_loss > 0:
            progress_msg += f" | ê²€ì¦ ì†ì‹¤: {val_loss:.4f}"
            
        # ì •í™•ë„ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
        acc = logs.get('accuracy', 0)
        val_acc = logs.get('val_accuracy', 0)
        progress_msg += f" | ì •í™•ë„: {acc:.4f}"
        if val_acc > 0:
            progress_msg += f" | ê²€ì¦ ì •í™•ë„: {val_acc:.4f}"
        
        logger.info(progress_msg)

class AugmentSwitchCallback(tf.keras.callbacks.Callback):
    """ìŠ¤í…Œì´ì§€ë³„ MixUp/CutMix ì ìš© ì—¬ë¶€ í† ê¸€"""
    def __init__(self, enable: bool, name: str = ""):
        super().__init__()
        self.enable = enable
        self.name = name
    def on_train_begin(self, logs=None):
        try:
            APPLY_MIX.assign(self.enable)
            logger.info(f"ğŸ›ï¸  AugmentSwitch: {self.name} | MixUp/CutMix {'í™œì„±í™”' if self.enable else 'ë¹„í™œì„±í™”'}")
        except Exception as e:
            logger.warning(f"âš ï¸  AugmentSwitch ì‹¤íŒ¨: {e}")

class EMACallback(tf.keras.callbacks.Callback):
    """ì§€ìˆ˜ì´ë™í‰ê· (EMA) ê°€ì¤‘ì¹˜ ì¶”ì  ë° ìŠ¤ì™‘

    ìŠ¤í…Œì´ì§€ë³„ë¡œ trainable ë³€ìˆ˜ êµ¬ì„±ì´ ë°”ë€Œë¯€ë¡œ(on/off freeze),
    ë§¤ í•™ìŠµ ì‹œì‘ë§ˆë‹¤ shadow ë³€ìˆ˜ ëª©ë¡ì„ ì¬êµ¬ì„±í•œë‹¤.
    """
    def __init__(self, decay: float = 0.999):
        super().__init__()
        self.decay = decay
        self.ema_vars = []
        self.orig_vars = []

    def _rebuild_from_model(self):
        self.ema_vars = [tf.Variable(v, trainable=False, dtype=v.dtype) for v in self.model.trainable_variables]

    def on_train_begin(self, logs=None):
        # í˜„ì¬ trainable_variablesì— ë§ì¶° shadow ë³€ìˆ˜ ì¬ìƒì„± ë° ì´ˆê¸°í™”
        self._rebuild_from_model()
        for ev, v in zip(self.ema_vars, self.model.trainable_variables):
            ev.assign(v)
        logger.info(f"âœ… EMA ì´ˆê¸°í™” ì™„ë£Œ (decay={self.decay}, vars={len(self.ema_vars)})")

    def on_train_batch_end(self, batch, logs=None):
        # ë§¤ ë°°ì¹˜ ì—…ë°ì´íŠ¸ (í˜„ì¬ trainable ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë™ì¼ ìˆœì„œ ê°€ì •)
        for ev, v in zip(self.ema_vars, self.model.trainable_variables):
            ev.assign(self.decay * ev + (1.0 - self.decay) * v)

    def apply_ema_weights(self):
        # í˜„ ì‹œì  trainable ëª©ë¡ê³¼ shadow ê°œìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ ì¬ì •ë ¬
        if len(self.ema_vars) != len(self.model.trainable_variables):
            self._rebuild_from_model()
        self.orig_vars = [tf.identity(v) for v in self.model.trainable_variables]
        for v, ev in zip(self.model.trainable_variables, self.ema_vars):
            v.assign(ev)
        logger.info("ğŸ”„ EMA ê°€ì¤‘ì¹˜ ì ìš©")

    def restore_original_weights(self):
        if not self.orig_vars:
            return
        for v, ov in zip(self.model.trainable_variables, self.orig_vars):
            v.assign(ov)
        logger.info("ğŸ”™ ì›ë˜ ê°€ì¤‘ì¹˜ë¡œ ë³µì›")

# ============ 0) í™˜ê²½: CPU/GPU ì„¤ì • ============
def setup_device():
    """CPU/GPU ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    physical_devices = tf.config.list_physical_devices()
    gpu_devices = tf.config.list_physical_devices('GPU')
    
    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {physical_devices}")
    
    if USE_GPU:
        if gpu_devices:
            logger.info(f"ğŸš€ GPU ëª¨ë“œ í™œì„±í™” - ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {len(gpu_devices)}ê°œ")
            for i, gpu in enumerate(gpu_devices):
                logger.info(f"   GPU {i}: {gpu}")
            
            # GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©
            try:
                for gpu in gpu_devices:
                    tf.config.experimental.set_memory_growth(gpu, True)
                logger.info("âœ… GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš© ì„¤ì • ì™„ë£Œ")
            except RuntimeError as e:
                logger.warning(f"âš ï¸  GPU ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨ (ì´ë¯¸ ì´ˆê¸°í™”ë¨): {e}")
            
            # Mixed Precision í™œì„±í™” 
            try:
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                logger.info("âœ… Mixed Precision (float16) í™œì„±í™” - Tensor Core ê°€ì†")
            except Exception as e:
                logger.warning(f"âš ï¸  Mixed Precision ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (ì„ íƒì‚¬í•­ - RTX 3060ì€ 12GB)
            # tf.config.experimental.set_memory_limit(gpu_devices[0], 10240)  # 10GBë¡œ ì œí•œ
            
        else:
            logger.warning("âš ï¸  GPU ì‚¬ìš©ì´ ìš”ì²­ë˜ì—ˆì§€ë§Œ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            tf.config.set_visible_devices([], 'GPU')
    else:
        logger.info("ğŸ–¥ï¸  CPU ëª¨ë“œ í™œì„±í™”")
        tf.config.set_visible_devices([], 'GPU')
    
    # ìµœì¢… ì‚¬ìš© ë””ë°”ì´ìŠ¤ í™•ì¸
    available_devices = tf.config.list_logical_devices()
    logger.info(f"ì‹¤ì œ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {available_devices}")

setup_device()

# ëœë¤ ì‹œë“œ ì„¤ì • (SEEDëŠ” ìƒë‹¨ì—ì„œ ì •ì˜ë¨)
tf.random.set_seed(SEED)
np.random.seed(SEED)
logger.info(f"Random seed set to {SEED}")


os.makedirs(OUT_DIR, exist_ok=True)
logger.info(f"Output directory created: {OUT_DIR}")

# ============ 1) í´ë˜ìŠ¤ ì •ë³´ ë¡œê¹… ë° ê²€ì¦ ============
logger.info(f"í´ë˜ìŠ¤(í™˜ê²½ë³€ìˆ˜): {CLASS_NAMES}, ì´ {NUM_CLASSES}ê°œ")

if NUM_CLASSES == 1:
    logger.info(f"ë‹¨ì¼ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ë¡œ í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤: {CLASS_NAMES[0]}")
else:
    logger.info(f"{NUM_CLASSES}ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ë¡œ í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤: {CLASS_NAMES}")

# train í´ë” ê²€ì¦
train_folders = sorted([d for d in os.listdir(DATA_DIR_TRAIN) if os.path.isdir(os.path.join(DATA_DIR_TRAIN, d))])
expected_folders = sorted(CLASS_NAMES)

if train_folders != expected_folders:
    logger.error(f"âŒ train í´ë” ë¶ˆì¼ì¹˜: ì‹¤ì œ {train_folders} â‰  ì˜ˆìƒ {expected_folders}")
    raise ValueError(f"Train folder mismatch: {train_folders} != {expected_folders}")

# validation í´ë” ê²€ì¦  
val_folders = sorted([d for d in os.listdir(DATA_DIR_VAL) if os.path.isdir(os.path.join(DATA_DIR_VAL, d))])

if val_folders != expected_folders:
    logger.error(f"âŒ validation í´ë” ë¶ˆì¼ì¹˜: ì‹¤ì œ {val_folders} â‰  ì˜ˆìƒ {expected_folders}")
    raise ValueError(f"Validation folder mismatch: {val_folders} != {expected_folders}")

# íŒŒì¼ ê°œìˆ˜ í™•ì¸
for class_name in CLASS_NAMES:
    train_count = len(os.listdir(os.path.join(DATA_DIR_TRAIN, class_name)))
    val_count = len(os.listdir(os.path.join(DATA_DIR_VAL, class_name)))
    logger.info(f"âœ… {class_name}: train {train_count}ê°œ, validation {val_count}ê°œ")

# ============ 2) ë°ì´í„° ë¡œë“œ ============
# ë¶„ë¥˜ ëª¨ë“œ: í•­ìƒ categorical ë¼ë²¨ ì‚¬ìš©
label_mode = "categorical"

# Train ë°ì´í„°ì…‹ ë¡œë“œ
raw_train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR_TRAIN,
    labels="inferred",
    label_mode=label_mode,
    color_mode="rgb",               # RGB ì…ë ¥
    batch_size=None,
    image_size=(IMG_SIZE, IMG_SIZE),
    shuffle=True,
    seed=SEED,
    class_names=CLASS_NAMES,
)

# Validation ë°ì´í„°ì…‹ ë¡œë“œ
raw_val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR_VAL,
    labels="inferred",
    label_mode=label_mode,
    color_mode="rgb",               # RGB ì…ë ¥
    batch_size=None,
    image_size=(IMG_SIZE, IMG_SIZE),
    shuffle=False,  # validationì€ ì…”í”Œí•˜ì§€ ì•ŠìŒ
    seed=SEED,
    class_names=CLASS_NAMES,
)

# class_names í™•ì¸
assert raw_train_ds.class_names == CLASS_NAMES
assert raw_val_ds.class_names == CLASS_NAMES
logger.info(f"ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìˆœì„œ í™•ì¸ ì™„ë£Œ: {raw_train_ds.class_names}")

# ============ 3) ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ============
_backbone_builder, _preprocess_input_fn = get_backbone_and_preprocess(IMG_SIZE)

def preprocess_single(x, y):
    return _preprocess_input_fn(x), y

# MixUp / CutMix êµ¬í˜„ (í•™ìŠµ ë°°ì¹˜ì—ë§Œ ì ìš©)
# ìœ„ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ ë…¸ì¶œëœ ê°’ì„ ì‚¬ìš© (ê¸°ë³¸ê°’ ë™ì¼)
def _sample_beta(alpha: float, beta: float, shape):
    a = tf.random.gamma(shape=shape, alpha=alpha, dtype=tf.float32)
    b = tf.random.gamma(shape=shape, alpha=beta, dtype=tf.float32)
    return a / (a + b)

def _mixup(images, labels, alpha=MIXUP_ALPHA):
    bs = tf.shape(images)[0]
    idx = tf.random.shuffle(tf.range(bs))
    images2 = tf.gather(images, idx)
    labels2 = tf.gather(labels, idx)
    lam = _sample_beta(alpha, alpha, [bs, 1, 1, 1])
    images_out = lam * images + (1.0 - lam) * images2
    lam_lbl = tf.reshape(lam, [bs, 1])
    labels_out = lam_lbl * labels + (1.0 - lam_lbl) * labels2
    return images_out, labels_out

def _cutmix(images, labels, alpha=CUTMIX_ALPHA):
    bs = tf.shape(images)[0]
    h = tf.shape(images)[1]
    w = tf.shape(images)[2]
    idx = tf.random.shuffle(tf.range(bs))
    images2 = tf.gather(images, idx)
    labels2 = tf.gather(labels, idx)

    lam = _sample_beta(alpha, alpha, [bs])  # (B,)
    cut_rat = tf.sqrt(1.0 - lam)
    cut_w = tf.cast(tf.cast(w, tf.float32) * cut_rat, tf.int32)
    cut_h = tf.cast(tf.cast(h, tf.float32) * cut_rat, tf.int32)

    # ëœë¤ ë°•ìŠ¤ ì¤‘ì‹¬
    cx = tf.random.uniform([bs], 0, w, dtype=tf.int32)
    cy = tf.random.uniform([bs], 0, h, dtype=tf.int32)

    x1 = tf.clip_by_value(cx - cut_w // 2, 0, w)
    y1 = tf.clip_by_value(cy - cut_h // 2, 0, h)
    x2 = tf.clip_by_value(cx + cut_w // 2, 0, w)
    y2 = tf.clip_by_value(cy + cut_h // 2, 0, h)

    # ë¸Œë¡œë“œìºìŠ¤íŒ…ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„± [B,H,W,1]
    yy = tf.reshape(tf.range(h, dtype=tf.int32), [1, h, 1, 1])
    xx = tf.reshape(tf.range(w, dtype=tf.int32), [1, 1, w, 1])
    y1b = tf.reshape(y1, [bs, 1, 1, 1])
    y2b = tf.reshape(y2, [bs, 1, 1, 1])
    x1b = tf.reshape(x1, [bs, 1, 1, 1])
    x2b = tf.reshape(x2, [bs, 1, 1, 1])
    mask_y = tf.logical_and(yy >= y1b, yy < y2b)
    mask_x = tf.logical_and(xx >= x1b, xx < x2b)
    masks = tf.cast(tf.logical_and(mask_y, mask_x), tf.float32)

    images_out = images * (1.0 - masks) + images2 * masks

    box_areas = tf.cast((y2 - y1) * (x2 - x1), tf.float32)
    lam_adj = 1.0 - (box_areas / tf.cast(h * w, tf.float32))  # ì‹¤ì œ ë¼ë²¨ ë¹„ìœ¨ ì¡°ì •
    lam_adj = tf.reshape(lam_adj, [bs, 1])
    labels_out = lam_adj * labels + (1.0 - lam_adj) * labels2
    return images_out, labels_out

def mix_augment(images, labels):
    r = tf.random.uniform([])
    def do_mixup():
        return _mixup(images, labels)
    def do_cutmix():
        return _cutmix(images, labels)
    return tf.cond(r < P_MIXUP, do_mixup, do_cutmix)

# ìŠ¤í…Œì´ì§€ë³„ MixUp/CutMix ì ìš© ì—¬ë¶€ë¥¼ ì œì–´í•  í”Œë˜ê·¸ (Callbackì—ì„œ ë³€ê²½)
APPLY_MIX = tf.Variable(True, dtype=tf.bool)

def maybe_mix_augment(images, labels):
    return tf.cond(APPLY_MIX, lambda: mix_augment(images, labels), lambda: (images, labels))

# ì…”í”Œ ë²„í¼ í¬ê¸° ê³„ì‚°
train_count = int(tf.data.experimental.cardinality(raw_train_ds).numpy())
SHUFFLE_BUF = min(train_count, max(2048, min(BATCH_SIZE * 128, 8192)))

# ë°ì´í„° ì…”í”Œ ë° íŒŒì´í”„ë¼ì¸ êµ¬ì„±
AUTOTUNE = tf.data.AUTOTUNE
train_ds = (
    raw_train_ds
    .shuffle(SHUFFLE_BUF, reshuffle_each_iteration=True)   # ì´ë¯¸ì§€ ë‹¨ìœ„ ì…”í”Œ
    .batch(BATCH_SIZE, drop_remainder=True)                # ì´í›„ ë°°ì¹˜
    .map(preprocess_single, num_parallel_calls=AUTOTUNE)
    .map(maybe_mix_augment, num_parallel_calls=AUTOTUNE)   # ì¡°ê±´ë¶€ MixUp/CutMix
    .prefetch(AUTOTUNE)
)
val_ds = (
    raw_val_ds
    .batch(BATCH_SIZE)                                     # ê²€ì¦ì€ ì…”í”ŒX
    .map(preprocess_single, num_parallel_calls=AUTOTUNE)
    .prefetch(AUTOTUNE)
)

# ë°˜ë³µ ëŒ€ì‹  ë°ì´í„°ì…‹ í¬ê¸° ê¸°ë°˜ ìŠ¤í… ê³„ì‚°
train_batches = int(tf.data.experimental.cardinality(train_ds).numpy())
val_batches = int(tf.data.experimental.cardinality(val_ds).numpy())
if AUTO_STEPS:
    STEPS_PER_EPOCH_HEAD = train_batches
    STEPS_PER_EPOCH_FINETUNE = train_batches
    val_steps = val_batches if val_batches else VAL_STEPS
    logger.info(f"AUTO_STEPS í™œì„±í™”: steps_per_epoch(train)={train_batches}, validation_steps={val_batches}")
else:
    val_steps  = min(VAL_STEPS, val_batches) if val_batches else VAL_STEPS

# ============ 4) ëª¨ë¸ êµ¬ì„± ============
base = _backbone_builder()
base.trainable = False  # 1ë‹¨ê³„: ë°±ë³¸(ëª¸í†µ) ë™ê²°, í—¤ë“œ(ë¨¸ë¦¬)ë§Œ í•™ìŠµ

inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))
# ë°ì´í„° ì¦ê°• ë ˆì´ì–´(í•™ìŠµì‹œì—ë§Œ ëœë¤ ë™ì‘)
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomTranslation(0.05, 0.05),
    layers.RandomContrast(0.1),
], name="data_augmentation")

# í—¤ë“œ í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” BNì„ ì¶”ë¡  ëª¨ë“œë¡œ ê³ ì •í•˜ì—¬ í†µê³„ì¹˜ê°€ í”ë“¤ë¦¬ì§€ ì•Šë„ë¡ ì•ˆì •í™”
x_aug = data_augmentation(inputs)
x = base(x_aug, training=False)
# GAP + GMP ê²°í•© í›„ LN, 2ë‹¨ í—¤ë“œ (Swish)
x_gap = layers.GlobalAveragePooling2D()(x)
x_gmp = layers.GlobalMaxPooling2D()(x)
x = layers.Concatenate()([x_gap, x_gmp])
x = layers.LayerNormalization()(x)
x = layers.Dense(512, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)
x = layers.Dropout(max(DROPOUT_RATE, 0.4))(x)
x = layers.Dense(256, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)
x = layers.Dropout(max(DROPOUT_RATE, 0.3))(x)

# ë¶„ë¥˜ ëª¨ë“œ: softmax ì¶œë ¥ (Mixed Precision ì‚¬ìš© ì‹œ float32 ì¶œë ¥ ìœ ì§€)
if USE_GPU and tf.keras.mixed_precision.global_policy().name == 'mixed_float16':
    outputs = layers.Dense(NUM_CLASSES, activation="softmax", dtype='float32')(x)
    logger.info(f"ë¶„ë¥˜ ëª¨ë¸ êµ¬ì„±: {NUM_CLASSES}í´ë˜ìŠ¤ ì¶œë ¥ (Mixed Precision - float32 ì¶œë ¥)")
else:
    outputs = layers.Dense(NUM_CLASSES, activation="softmax")(x)
    logger.info(f"ë¶„ë¥˜ ëª¨ë¸ êµ¬ì„±: {NUM_CLASSES}í´ë˜ìŠ¤ ì¶œë ¥")

model = models.Model(inputs, outputs)

# ============ 5) ì»´íŒŒì¼(í•™ìŠµ ê·œì¹™) ============
def make_warmup_cosine_lr(base_lr: float, steps_per_epoch: int, epochs: int, warmup_ratio: float = 0.1):
    total_steps = max(1, steps_per_epoch * max(1, epochs))
    warmup_steps = int(total_steps * max(0.0, min(1.0, warmup_ratio)))
    cosine_steps = max(1, total_steps - warmup_steps)

    cosine = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=base_lr, decay_steps=cosine_steps)

    class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):
        def __init__(self, warmup_steps, base_lr, cosine):
            self.warmup_steps = warmup_steps
            self.base_lr = base_lr
            self.cosine = cosine
        def __call__(self, step):
            step = tf.cast(step, tf.float32)
            if self.warmup_steps > 0:
                warmup_lr = self.base_lr * (step / float(self.warmup_steps))
                decay_lr = self.cosine(tf.maximum(0.0, step - self.warmup_steps))
                return tf.where(step < self.warmup_steps, warmup_lr, decay_lr)
            else:
                return self.cosine(step)
    return WarmupCosine(warmup_steps, base_lr, cosine)

lr_head = make_warmup_cosine_lr(LEARNING_RATE_HEAD, STEPS_PER_EPOCH_HEAD, EPOCHS_HEAD, WARMUP_RATIO)
optimizer_head = tf.keras.optimizers.AdamW(learning_rate=lr_head, weight_decay=WEIGHT_DECAY, clipnorm=1.0)

model.compile(
    optimizer=optimizer_head,
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),
    metrics=["accuracy"],
)
logger.info("ë¶„ë¥˜ ëª¨ë¸ë¡œ ì»´íŒŒì¼ ì™„ë£Œ")

# ì „ì—­ ë² ìŠ¤íŠ¸(ëª¨ë“  ìŠ¤í…Œì´ì§€ ê³µìš©) ì²´í¬í¬ì¸íŠ¸ - val_loss ê¸°ì¤€
best_overall_path = os.path.join(OUT_DIR, "best_overall.weights.h5")
best_overall_ckpt = ModelCheckpoint(
    filepath=best_overall_path,
    monitor="val_loss",
    mode="min",
    save_best_only=True,
    save_weights_only=True,
)

ema_callback = EMACallback(decay=float(os.getenv('EMA_DECAY')))

# ì²´í¬í¬ì¸íŠ¸(í—¤ë“œ ë‹¨ê³„): ê°€ì¤‘ì¹˜ë§Œ HDF5ë¡œ ì €ì¥ â†’ TF2.13 í˜¸í™˜ ì•ˆì „
callbacks_head = [
    EarlyStopping(monitor="val_accuracy", patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True),
    ModelCheckpoint(
        filepath=os.path.join(OUT_DIR, "best_head.weights.h5"),
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,
    ),
    best_overall_ckpt,
    SimpleRealTimeCallback("í—¤ë“œ í•™ìŠµ"),  # ê°„ë‹¨í•œ ì‹¤ì‹œê°„ ì‹œê°í™”
    AugmentSwitchCallback(True, name="Head"),
    ema_callback,
]

logger.info("=== [1ë‹¨ê³„] í—¤ë“œ í•™ìŠµ ì‹œì‘ ===")
def compute_class_weights(data_dir, class_names):
    counts = []
    for name in class_names:
        counts.append(len(os.listdir(os.path.join(data_dir, name))))
    total = sum(counts)
    weights = {}
    for idx, c in enumerate(counts):
        weights[idx] = total / (len(counts) * max(1, c))
    logger.info(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {weights}")
    return weights

class_weights = compute_class_weights(DATA_DIR_TRAIN, CLASS_NAMES)

history_head = model.fit(
    train_ds,
    validation_data=val_ds,
    steps_per_epoch=STEPS_PER_EPOCH_HEAD,
    validation_steps=val_steps,
    epochs=EPOCHS_HEAD,
    callbacks=callbacks_head,
    class_weight=class_weights,
)
logger.info("í—¤ë“œ í•™ìŠµ ì™„ë£Œ!")

# ============ 6) íŒŒì¸íŠœë‹: ë‹¨ê³„ì  ì–¸í”„ë¡œì¦Œ(3ë‹¨ê³„) ============
def freeze_bn(backbone):
    """Backbone ë‚´ BatchNormalization ë ˆì´ì–´ë§Œ ë™ê²°(trainable=False)"""
    cnt = 0
    for l in backbone.layers:
        if isinstance(l, tf.keras.layers.BatchNormalization):
            l.trainable = False
            cnt += 1
    return cnt
    
def set_unfreeze_ratio(backbone, ratio: float):
    """ëª¨ë¸ ë’¤ìª½ ratio(0~1) ë§Œí¼ë§Œ trainable=True ë¡œ ì„¤ì • (ì˜ˆ: 0.3 â†’ ë’¤ 30%)"""
    n = len(backbone.layers)
    cut = int(n * (1.0 - ratio))  # cut ì´ì „: ë™ê²°(False), cut ì´í›„: í•™ìŠµ(True)
    for i, l in enumerate(backbone.layers):
        l.trainable = (i >= cut)

def compile_for_ft(lr: float, steps_per_epoch: int, epochs: int):
    """trainable ë³€ê²½ í›„ ì¬-ì»´íŒŒì¼ (í•„ìˆ˜)"""
    lr_schedule = make_warmup_cosine_lr(lr, steps_per_epoch, epochs, WARMUP_RATIO)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=WEIGHT_DECAY, clipnorm=1.0)
    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),
        metrics=["accuracy"],
    )

def make_ft_callbacks(stage_name: str, enable_mix: bool = True):
    return [
        EarlyStopping(monitor="val_accuracy",
                      patience=EARLY_STOPPING_PATIENCE,
                      restore_best_weights=True),
        ModelCheckpoint(
            filepath=os.path.join(OUT_DIR, f"best_finetune_{stage_name}.weights.h5"),
            monitor="val_accuracy",
            save_best_only=True,
            save_weights_only=True,
        ),
        best_overall_ckpt,
        SimpleRealTimeCallback(f"íŒŒì¸íŠœë‹-{stage_name}"),
        AugmentSwitchCallback(enable_mix, name=stage_name),
        ema_callback,
    ]

# ì´ FT ì—í¬í¬ë¥¼ 3ë“±ë¶„ (ì˜ˆ: 42 â†’ 14/14/14)
ft_total = EPOCHS_FINETUNE
stage_epochs = [ft_total // 3, ft_total // 3, ft_total - 2 * (ft_total // 3)]  # í•©=ft_total

# ìŠ¤í…Œì´ì§€ë³„ ì–¸í”„ë¡œì¦Œ ë¹„ìœ¨ & ëŸ¬ë‹ë ˆì´íŠ¸ (ê°ì‡  ì™„í™”: 1.0 â†’ 0.75 â†’ 0.5)
ratios = [0.50, 0.75, 1.00]  # ê¶Œì¥: ë’¤ 50% â†’ 75% â†’ ì „ì¸µ
lr1 = LEARNING_RATE_FINETUNE
lr2 = max(1e-6, LEARNING_RATE_FINETUNE*0.75)
lr3 = max(5e-7, LEARNING_RATE_FINETUNE*0.5)
lrs = [lr1, lr2, lr3]

logger.info("=== [2ë‹¨ê³„] íŒŒì¸íŠœë‹(ë‹¨ê³„ì  ì–¸í”„ë¡œì¦Œ) ì‹œì‘ ===")
for idx, (ratio, epochs, lr) in enumerate(zip(ratios, stage_epochs, lrs), start=1):
    stage_name = f"s{idx}_ratio{int(ratio*100)}"
    logger.info(f"â–¶ Stage {idx}: ë’¤ {int(ratio*100)}% ì–¸í”„ë¡œì¦Œ | epochs={epochs}, lr={lr:g}")

    # 1) ì–¸í”„ë¡œì¦Œ ë²”ìœ„ ì„¤ì •
    base.trainable = True
    set_unfreeze_ratio(base, ratio)

    # BN ì •ì±… ì ìš©
    if FREEZE_BN_MODE == 'all':
        frozen = freeze_bn(base)
        logger.info(f"[{stage_name}] BN policy=all, frozen: {frozen}")
    elif FREEZE_BN_MODE == 's3':
        if idx < 3:
            frozen = freeze_bn(base)
            logger.info(f"[{stage_name}] BN policy=s3, frozen: {frozen}")
        else:
            logger.info(f"[{stage_name}] BN policy=s3, BN trainable")
    elif FREEZE_BN_MODE == 'adaptive':
        if idx == 1:
            frozen = freeze_bn(base)
            logger.info(f"[{stage_name}] BN policy=adaptive(s1 freeze), frozen: {frozen}")
        elif idx == 2:
            # s2: ë§ˆì§€ë§‰ 20% ë ˆì´ì–´ì˜ BNë§Œ í•™ìŠµ í—ˆìš©
            n = len(base.layers)
            cut = int(n * 0.8)
            frozen = 0
            for i, l in enumerate(base.layers):
                if isinstance(l, tf.keras.layers.BatchNormalization):
                    l.trainable = (i >= cut)
                    if not l.trainable:
                        frozen += 1
            logger.info(f"[{stage_name}] BN policy=adaptive(s2 partial unfreeze), frozen: {frozen}")
        else:
            logger.info(f"[{stage_name}] BN policy=adaptive(s3 all BN trainable)")
    else:
        logger.info(f"[{stage_name}] BN policy=none, BN trainable")

    # 2) ì¬-ì»´íŒŒì¼ (trainable ë³€ê²½ í›„ ë°˜ë“œì‹œ í•„ìš”)
    compile_for_ft(lr, STEPS_PER_EPOCH_FINETUNE, epochs)

    # 3) ìŠ¤í…Œì´ì§€ ì „ìš© ì½œë°±
    enable_mix = not (DISABLE_MIX_IN_LAST_STAGE and idx == 3)
    callbacks_stage = make_ft_callbacks(stage_name, enable_mix=enable_mix)

    # 4) í•™ìŠµ
    history_ft = model.fit(
        train_ds,
        validation_data=val_ds,
        steps_per_epoch=STEPS_PER_EPOCH_FINETUNE,
        validation_steps=val_steps,
        epochs=epochs,
        callbacks=callbacks_stage,
        class_weight=class_weights,
    )

logger.info("íŒŒì¸íŠœë‹(ë‹¨ê³„ì  ì–¸í”„ë¡œì¦Œ) ì™„ë£Œ!")

# ============ 7) í‰ê°€ ============
# ì „ì—­ ë² ìŠ¤íŠ¸ë¡œ ë³µì› í›„ í‰ê°€ (s1/s2ê°€ ë” ì¢‹ì•˜ë˜ ê²½ìš°ë„ ìë™ ë°˜ì˜)
if os.path.exists(best_overall_path):
    model.load_weights(best_overall_path)
    logger.info("âœ… ì „ì—­ ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ë¡œ ë³µì› ì™„ë£Œ (monitor=val_loss)")

val_loss, val_acc = model.evaluate(val_ds, verbose=0)
logger.info(f"ìµœì¢… ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")

# ê°„ë‹¨ TTA(Flip) í‰ê°€
def evaluate_tta_flip(model, dataset):
    total = 0
    correct = 0
    for xs, ys in dataset:
        probs1 = model.predict(xs, verbose=0)
        probs2 = model.predict(tf.image.flip_left_right(xs), verbose=0)
        probs = (probs1 + probs2) / 2.0
        preds = np.argmax(probs, axis=1)
        trues = np.argmax(ys.numpy(), axis=1)
        correct += np.sum(preds == trues)
        total += xs.shape[0]
    return correct / max(1, total)

tta_acc = evaluate_tta_flip(model, val_ds)
logger.info(f"TTA(Flip) ê²€ì¦ ì •í™•ë„: {tta_acc:.4f}")

# EMA ê°€ì¤‘ì¹˜ë¡œ ì¬í‰ê°€ í›„ ë” ë‚˜ì€ ìª½ ìœ ì§€
try:
    ema_callback.apply_ema_weights()
    val_loss_ema, val_acc_ema = model.evaluate(val_ds, verbose=0)
    logger.info(f"EMA ê°€ì¤‘ì¹˜ ê²€ì¦ ì •í™•ë„: {val_acc_ema:.4f}")
    if val_acc_ema >= val_acc:
        logger.info("ğŸ“Œ EMA ê°€ì¤‘ì¹˜ë¥¼ ìµœì¢… ê°€ì¤‘ì¹˜ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.")
    else:
        ema_callback.restore_original_weights()
except Exception as e:
    logger.warning(f"âš ï¸  EMA í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

# ============ 8) ìµœì¢… ì €ì¥ ============
model_name = f"emotion_classifier_{NUM_CLASSES}_classes"

# (A) TensorFlow SavedModel(í´ë”)
savedmodel_path = os.path.join(OUT_DIR, f"{model_name}_savedmodel")
model.export(savedmodel_path)
logger.info(f"SavedModel ì €ì¥ ì™„ë£Œ: {savedmodel_path}")

# (B) HDF5 ë‹¨ì¼ íŒŒì¼
h5_path = os.path.join(OUT_DIR, f"{model_name}.h5")
# ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„(WarmupCosine)ì´ í¬í•¨ëœ ì˜µí‹°ë§ˆì´ì €ëŠ” ì§ë ¬í™”ê°€ ì–´ë ¤ìš°ë¯€ë¡œ
# ì˜µí‹°ë§ˆì´ì € ì œì™¸ ì €ì¥ìœ¼ë¡œ í˜¸í™˜ì„± í™•ë³´
model.save(h5_path, include_optimizer=False)
logger.info(f"HDF5 ì €ì¥ ì™„ë£Œ: {h5_path}")

# ============ ë¹ ë¥¸ ì˜ˆì¸¡ ë°ëª¨ ============
# ê²€ì¦ì…‹ì€ ì…”í”Œí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì‹œê°í™”/ì ê²€ìš© ë°°ì¹˜ëŠ” ëœë¤ ìƒ˜í”Œë¡œ êµ¬ì„±
def take_random_val_batch(dataset, batch_size, seed=SEED):
    return dataset.unbatch().shuffle(8192, seed=seed, reshuffle_each_iteration=True).batch(batch_size).take(1)

for xs, ys in take_random_val_batch(val_ds, BATCH_SIZE):
    probs = model.predict(xs, verbose=0)   # (B, NUM_CLASSES)
    preds = np.argmax(probs, axis=1)
    trues = np.argmax(ys.numpy(), axis=1)
    logger.info("ìƒ˜í”Œ ì˜ˆì¸¡:")
    logger.info(f"ì˜ˆì¸¡: {[CLASS_NAMES[i] for i in preds]}")
    logger.info(f"ì •ë‹µ: {[CLASS_NAMES[i] for i in trues]}")
    break

logger.info("ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥ ì™„ë£Œ!")

# matplotlib ì‹œê°í™” ë§ˆë¬´ë¦¬ (ì‹œê°í™”ê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
if ENABLE_VISUALIZATION:
    logger.info("âœ… ì‹¤ì‹œê°„ ì‹œê°í™” ì™„ë£Œ!")
    logger.info("ğŸ’¡ ê·¸ë˜í”„ ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì°½ì„ ì§ì ‘ ë‹«ìœ¼ì„¸ìš”.")
    
    # ê·¸ë˜í”„ ì°½ì´ ì—´ë ¤ìˆëŠ” ë™ì•ˆ í”„ë¡œê·¸ë¨ ìœ ì§€
    try:
        plt.show(block=True)  # ê·¸ë˜í”„ ì°½ì´ ë‹«í ë•Œê¹Œì§€ ëŒ€ê¸°
    except KeyboardInterrupt:
        logger.info("âŒ¨ï¸  ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"âš ï¸  ì‹œê°í™” ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        plt.close('all')  # ëª¨ë“  ê·¸ë˜í”„ ì°½ ë‹«ê¸°
        logger.info("ğŸ”š ì‹œê°í™” ì¢…ë£Œ")
else:
    logger.info("ğŸ“Š CLI ëª¨ë“œ í•™ìŠµ ì™„ë£Œ!")
